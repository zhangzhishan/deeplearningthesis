% !Mode:: "TeX:UTF-8"

\chapter{深度卷积神经网络}
\label{sec:network}
\section{引言}

人工神经网络\ucite{hebb2005organization}是一种通过模仿大脑神经元行为进行信息处理的数学模型，但其无法承受大规模的参数和训练样本并且具有泛化能力差等问题。Fukushima\ucite{fukushima1982neocognitron}于1982年首次提出了卷积神经网络模型，Lecun 等人对神经网络传统算法在训练上面临的计算复杂度高等问题进行了改进，提出了基于梯度下降的优化算法\ucite{lecun1998gradient} 和BP算法\ucite{lecun1989backpropagation}。2003年，Simard对卷积神经网络进行了简化\ucite{simard2003best}。Hinton 在2006年的两篇文章\ucite{hinton2006reducing,hinton2006fast}可以作为深度学习（Deep Learning, DL）正式提出的里程碑。Ciren等人\ucite{ciresan2011flexible}在2011年对神经网络进行改造使其可以通过GPU训练计算，并在大量的图像数据集对深度学习算法进行了测试，且都取得了最好的成绩。其在工业界以及学术界掀起了巨大的浪潮，被应用于语音识别\ucite{hinton2012deep}、图像识别\ucite{krizhevsky2012imagenet}和自然语音处理\ucite{collobert2011natural}等各种方面。专门用于深度学习的芯片不断问世，例如谷歌公司的TPU\ucite{jouppi2017datacenter}，大大提高了深度学习的应用场景。

深度神经网络模型是一种非常强大的深度学习模型，它同时可以处理有监督和无监督学习任务。并且随着科技的发展，数据量越来越大，计算机并行能力也有了很大的提高。针对海量数据，简单的线性模型由于无法充分利用计算能力，不再适用，可以预见在将来会有越来越多的工作应用到深度学习。目前，其内涵已经超出了传统的多层神经网络，甚至机器学习的范畴，逐渐朝着人工智能的方向快速发展\ucite{silver2017mastering}。

本章安排如下：\ref{sec:network_classification}节对深度学习的基本分类进行了介绍，\ref{sec:neural}节详细地阐述了传统人工神经网络结构的定义和神经元之间信息传递的过程，\ref{sec:cnn}节对本文主要运用的深度卷积神经网络的特性进行了介绍，\ref{sec:network_summary}节进行本章总结。

\section{基本分类}
\label{sec:network_classification}
传统上可以把深度学习分为卷积神经网络、递归神经网络（Recurrent Neural Networks， RNN）、长短时记忆网络（Long short-term memory, LSTM）、深度信念网络（Deep Belief NetWorks, DBN）、自编码器、稀疏编码（Sparse Coding）、受限玻尔兹曼机（Restricted Boltzmann Machine, RBM）等。

其中卷积神经网络是最流行的一种深度学习模型，通过使用卷积层极大地减少了中间层的参数数目，使学习效率更高并减少过拟合，同时卷积操作独有的局部感受野（local receptive fields）、共享权重（shared weights）和池化（pooling）三种特性也是处理序列元素分类识别的很重要的一点，权重共享策略减少了需要训练的参数，相同的权重可以让卷积核不受信号位置的影响来检测信号的特性，使得训练出来的模型泛化能力更强；池化运算可以降低网络的空间分辨率，从而消除信号的微小偏移和扭曲。

递归神经网络是一种包含循环的，允许信息持久化的神经网络模型。传统的前馈神经网络中，单独的输入完全确定了余下层的神经元的激活值。而递归神经网络对的隐层和输出层的神经元的激活值不仅由当前的网络输入决定，而且包含了前面的输入的影响。长短时记忆网络是一种特殊的递归神经网络，主要用于解决递归神经网络前期模型难以训练的问题。其通过刻意设计的单元结构，在递归神经网络的基础上添加了元胞状态（cell state）用来保存长期的状态，然后通过门函数来控制此长期状态。

深度信念网络是一个概率生成模型，是由多个受限玻尔兹曼机组成，这些网络被“限制”为一个可见层和一个隐层，层间存在连接，但是层内的单元间不存在连接，通过训练隐层单元来捕捉在可见层表现出来的高阶数据的相关性。

通过对以上几种最常用的深度学习方法的介绍，易知卷积神经网络最适合处理本文这种静态类型的数据，循环或者说是不同时刻的输入对地海杂波类型的识别并没有提高，故递归神经网络和长短时记忆网络显然不适合本问题。另一方面，深度信念网络的生成模型并不关心不同类别之间的最优分类面的位置，故其用于分类问题时，分类精度没有判别模型高。且其学习的是数据的联合分布，相比其他算法具有更高的复杂性。

\section{传统人工神经网络结构}
\label{sec:neural}
人工神经网络是一种模拟大脑神经系统的算法，其可以从海量的训练样本中学习到一个权重函数，用来进行模式识别、分类等。神经网络主要是将许多个单一神经元（也称作感知器）连接在一起，一个神经元的输出就可以是另一个神经元的输入，形成一个有向无环的网络结构。
首先介绍单一神经元的结构，如图\ref{fig:neural}所示，神经元一般具有多个输入$x=[x_1,x_2,\dots,x_n] $，这些输入可以取0和1中的任意值。神经元对每一个输入有权重$w=[w_1,w_2,\dots,w_n] $ 和一个总的偏置$b_0$ ，其输出为:
\begin{equation}
y = f(w\cdot x + b_0),
\label{equ:neural}
\end{equation}
其中， $f$为该神经元的激活函数。

\begin{figure}[hbt]
  \centering   \sWuhao
  \includestandalone{figures/networks/neural}
  \caption{神经元}
  \label{fig:neural}
\end{figure}

上述多个神经元分层互联即可形成神经网络，图\ref{fig:network}是一个简单的神经网络，其中圆圈表示神经网络的节点，也即神经元。
对于一个具有$n_l$层的神经网络，如将第$ l $层用 $ L_l$表示，则有最左边的一层$  L_1$为输入层，最右的一层$ L_{n_l} $为输出层。由于不能在训练样本集中观测到中间所有节点的值，故将这些层称做隐层。
本例中网络的层数$n_l=3$，输入层具有4个输入单元，隐层具有5个隐层单元，输出层只有一个输出单元。

\begin{figure}[hbt]
\centering
\sWuhao
\includestandalone{figures/networks/network1}
\caption{神经网络}
\label{fig:network}
\end{figure}

根据输入 $x_i$求解输出$y$的主要过程为前向传播。
用$ w^{(l)}_{ij}$表示第 $ l $层的第 $ j$ 单元与第 $l+1 $层的第$i$ 单元之间的权重，$ b^{(l)}_i $是第$l$层中第$i$ 单元的偏置项。
因此，$w^{(l)}$为第$l$层权重组成的向量，$ w^{(1)} \in \mathbb{R}^{4\times 3}$ ， $ w^{(2)} \in \mathbb{R}^{1\times 5}$ 。
根据图\ref{fig:neural}和\equref{equ:neural}可以知道每个神经元节点的输出是经过激活函数的激活值，可以用$ y^{(l)}_i$ 表示第$  l $层的第 $ i$ 单元的输出值。
当$  l=1$ 时，$  y^{(1)}_i$为就是第$  i $个输入值，也即有$  y^{(1)}_i = x_i $。
当权重已知时，可以根据\equref{equ:neural}求解此传播过程，进而求解最终的输出值$y$。
从$L_1$层到$L_2$层的过程为：
\begin{align}
  y_1^{(2)} &= f(w_{11}^{(1)}x_1 + w_{12}^{(1)} x_2 + w_{13}^{(1)} x_3 + w_{14}^{(1)} x_4 + b_1^{(1)}),  \\
  y_2^{(2)} &= f(w_{21}^{(1)}x_1 + w_{22}^{(1)} x_2 + w_{23}^{(1)} x_3 + w_{24}^{(1)} x_4 + b_2^{(1)}),  \\
  y_3^{(2)} &= f(w_{31}^{(1)}x_1 + w_{32}^{(1)} x_2 + w_{33}^{(1)} x_3 + w_{34}^{(1)} x_4 + b_3^{(1)}),
\end{align}
\begin{align}
  y_4^{(2)} &= f(w_{41}^{(1)}x_1 + w_{42}^{(1)} x_2 + w_{43}^{(1)} x_3 + w_{44}^{(1)} x_4 + b_4^{(1)}),  \\
  y_5^{(2)} &= f(w_{51}^{(1)}x_1 + w_{52}^{(1)} x_2 + w_{53}^{(1)} x_3 + w_{54}^{(1)} x_4 + b_5^{(1)}).
  \label{equ:fp1}
\end{align}
根据同样的计算过程可以得到：
\begin{equation}
  y = y_1^{(3)} =  f(w_{11}^{(2)} y_1^{(2)} + w_{12}^{(2)} y_2^{(2)} + w_{13}^{(2)} y_3^{(2)} + w_{14}^{(2)} y_4^{(2)} + w_{15}^{(2)} y_5^{(2)} + b_1^{(2)}).
  \label{equ:fp2}
\end{equation}
把\equref{equ:fp1}代入\equref{equ:fp2}就可以计算得到输入$x_i$与输出$y$之间的关系。
为了使表达更清晰，用$ z^{(l)}_i $表示第$ l $层第$  i$单元输入加权和，则有：
\begin{equation}
  z_i^{(l)} = \sum_{j=1}^n w^{(l-1)}_{ij} x_j + b^{(l)}_i.
\end{equation}
因此\equref{equ:fp1}变为：
\begin{equation}
  y^{(l)}_i = f(z^{(l)}_i).
\end{equation}
则整个前向传播过程可以简化为：
\begin{align}
  z^{(l+1)} &= w^{(l)} y^{(l)} + b^{(l)}   \\
  h^{(l+1)} &= f(z^{(l+1)}),
\end{align}
其中，$b^{(l)}$为第$l+1$层偏置项组成的向量，$h^{(l)}$为第$l$层各个单元输出值组成的向量。

根据上述计算过程，对于一个具有 $n_l$ 层的前馈神经网络（在网络中没有闭环或者回路），其第$   1 $层是输入层，第$n_l$ 层是输出层，中间的所有隐层 $L_l$ 均与下一层 $L_{l+1}$全连接。
只需逐个计算每一个层中的节点的输出值，然后据此得到下一层的输出值。
一个复杂的神经网络一般会具有多个隐层并且输出层中可以有多个输出单元，
图\ref{fig:network2}的神经网络有三层隐层：$  L_2 $、$ L_3$及 $ L_4$ ，输出层$  L_5 $有三个输出单元。

\begin{figure}[hbt]
  \centering
  \sWuhao
  \includestandalone{figures/networks/network2}
  \caption{具有多个输出单元的神经网络示意图}
  \label{fig:network2}
\end{figure}

\section{深度卷积神经网络}
\label{sec:cnn}
卷积神经网络是深度学习中的一种重要算法，在分类等很多领域具有很大的优势，该方法经常被用于图像识别、语音识别等问题。
卷积神经网络是一种特殊的人工神经网络，由多对卷积层（Convolutional layer）和池化层（Pooling layer），以及全连接层（Full-connected layer）组成。图\ref{fig:cnn_network}为一个典型的卷积神经网络，最左边的为输入层，其中的神经元为输入神经元。最右边的输出层包含有一个或者多个输出神经元，其余的是多组卷积、池化层和有限数量的完全连接的隐层。

网络的前向传播主要是卷积和池化操作，其网络参数的更新采用误差反向传播（Error Backpropagation）算法。相比于传统人工网络，卷积神经网络有以下特点：
\begin{itemize}
  \item 输入为原始数据，对原始数据进行卷积运算，然后提取从最后一步生成的卷积数据的特征，丰富了识别中使用的特征。
  \item 多个卷积核可以利用输入数据中的局部结构，将整个输入空间划分成很小的隐层单元。将各个隐层单元的权重构建得到的卷积核作用于整个输入空间，从而得到特征向量。这种机制不仅大大减少了参数数量同时提高了数据的平移不变性。
  \item 具有超过两个的隐层以及可以逐层初始化模型的参数，通过更多的隐层可以更好地对事物的特征进行学习表达。
\end{itemize}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=13.5cm]{figures/networks/alexnetarchitecture}
  \caption{ AlexNet 卷积神经网络模型\ucite{krizhevsky2012imagenet}}
  \label{fig:cnn_network}
\end{figure}

\subsection{卷积}

深度卷积神经网络在特征提取过程中一个主要操作为卷积，在前向计算过程中，输入的一定区域的隐层的特征向量$x$和滤波器（权重）$w$ 点乘后得到新的特征向量，然后滑过一个个滤波器，以产生输出特征向量。
权重实际上是一个四维张量，第一维$ F $是卷积输入层的特征向量的数量，另一维$ F_p $是卷积输出层的特征向量的数量，另外两维是在宽度和高度方向给出的卷积核的大小，也称作感受野。

传统神经网络的每个输入元素会连接到每个隐层神经元。然而，在卷积神经网络中，只是把输入数据进行小的、局部区域的连接，也即第一个隐层中的每个神经元会连接到一个输入神经元的一个区域。这个输入向量的区域被称为隐层神经元的局部感受野，它是输入向量上的一个小窗口，对每个连接学习一个权重而隐层神经元同时学习一个总的偏置。通过在整个输入频谱数据上交叉移动局部感受野，可以构建起第一个隐层。

感受野允许用一个子集来代替整个输入数据。它的目的是在输入数据中搜索相似的模式，而不管模式在哪里，也即平移不变性。输出图像的宽度和高度也由步长（stride）确定，步长为在进行再次应用卷积运算之前在垂直、水平方向上滑动的像素的数目。以图\ref{fig:conv}展示的二维卷积操作来介绍卷积运算的过程。

\begin{figure}[hbt]
	\centering
  \includegraphics[width=13.5cm]{figures/networks/conv}
	\caption{卷积示意图}
	\label{fig:conv}
\end{figure}

图中的 $R_C$ 为卷积感受野的大小，$S_C$为卷积的步长。
根据输入高度$ T $和宽度$ N $，可以计算输出图像的高度和宽度：
\begin{align}
  N_p&=\frac{N+2P-R_C}{S_C}+1 \;,&
  T_p&=\frac{T+2P-R_C}{S_C}+1.
\end{align}
通常对输入数据进行边界扩充（padding）来使得输入特征图的宽度和高度满足$ N = N_p = T = T_p $，这样当前进步长$ S_C = 1 $ 时，可以得到：
\begin{align}
P&=\frac{R_C-1}{2}.
\end{align}
一个给定的层$ L_n $的卷积运算的计算过程为：
\begin{align}
y_{f\,l\,m}^{(\nu)}&=\sum^{F_\nu-1}_{f'=0}\sum^{R_C-1}_{j=0}\sum^{R_C-1}_{k=0}
w^{(o)f}_{f'\,j\,k}h^{(\nu)}_{f'\,S_Cl+j\,S_Cm+k},
\end{align}
其中$ o $表示网络中的第$ o + 1 $个卷积核。
$\nu \in [0,N-1 ]$ 表示网络中的第 $\nu$个隐层，$f\in[0,F_{\nu+1}-1]$为输入卷积层的第$f$个特征图， $l\in[0,N_{\nu+1}-1 ]$为输入卷积层的第$v$个特征图的宽度单位， $m\in[0,T_{\nu+1}-1 ]$为输入卷积层的第$v$个特征图的高度单位。 因此可以得到 $S_Cl+j\in[0,N_\nu-1 ]$， $S_Cl+m\in[0,T_\nu-1 ]$。于是可以通过添加激活函数计算隐层单元的输出。
考虑到边界填充可以得到：
\begin{align}
h_{f\,l+P\,m+P}^{(\nu+1)}&=f\left(y_{f\,l\,m}^{(\nu)}\right).
\end{align}
每个滤波器只关心数据的部分特征，当出现它学习到的特征时，就会呈现激活态。

\subsection{共享权重和偏置}
如前节所述，每个隐层神经元具有一个偏置和连接到它的局部感受野的权重，对该层的每一个神经元使用相同的权重和偏置，那么第$j$个隐层神经元的输出为：
\begin{equation}
  h_j = f(b+\sum_{m=1}^M w_m y_{j+m}),
  \label{equ:shared_weight}
\end{equation}
其中，$f$是神经元的激活函数，$b$是偏置的共享值，$w_m$是一个共享权重的$1\times M$向量，$y_k$表示位置$k$的输入激活值。
这意味着第一个隐层的所有神经元检测完全相同的特征，只是在输入数据的不同位置，因此卷积网络可以很好地适应数据偏移的情况。

\subsection{池化}
在通过卷积获得了特征之后，下一步希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如Softmax分类器（多分类的逻辑回归分类器），但这样做面临着计算量的挑战，除此以外过多的特征向量，也容易导致过拟合。
由于本文的雷达信号具有一种“静态性”属性，在一个数据区域有用的特征极有可能在另一个区域同样适用。因此，为了描述数据量较多的数据，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，可以计算频谱数据上一段频率范围内的某个特征的最大值（或平均值）。
这些经过采样的统计特征相比使用所有提取得到的特征不仅具有低得多的维度，而且不容易过拟合，在一定程度上会改善结果。这种聚合的操作称为池化。
池化操作是卷积神经网络中一种常用的用来对数据进行降维处理的方法。一般有平均池化和最大池化，主要过程为选取输入特征向量的$F$一个池化感受野$R_P$和步长$S_P$选取其最大或者平均元素来代表该区域，得到输出特征向量$F_p=F$，其中宽度 $N_p<N$，高度 $T_p<T$。在池化操作过程中，一般不考虑输入隐层的边界扩充值，因此在\equref{equ:pool}中存在 $+P$ 这一部分。

给定的第$\nu$个平均池化操作的公式为：
\begin{align}
  y_{f,l,m}^{(\nu)}&=\sum^{R_P-1}_{j,k=0} h_{f,S_Pl+j+P,S_Pm+k+P}^{(\nu)}.
  \label{equ:pool}
\end{align}
最大池化的公式为：
\begin{align}
  y_{f,l,m}^{(\nu)}&=\max_{j,k \in \{0, \ldots, R_P-1\}} h_{f,S_Pl+j+P,S_Pm+k+P}^{(\nu)},
  \label{equ:maxpool}
\end{align}
其中，$\nu \in [0,N-1 ]$ 表示网络中的第 $\nu$个隐层，$f\in[0,F_{\nu+1}-1]$, $l\in[0,N_{\nu+1}-1 ]$ 以及 $m\in[0,T_{\nu+1}-1 ]$。因此有 $S_Pl+j\in[0,N_\nu-1 ]$， $S_Pl+m\in[0,T_\nu-1 ]$。
最大池化是最常用的一种池化方式，本文所用的神经网络结构均使用最大池化。
用$j^{(t)(p)}_{_{f,l,m}},\,k^{(t)(p)}_{_{f,l,m}}$来表示第$t$次批采样在 $l,m$处特征向量图的最大值，则\equref{equ:maxpool} 变为：
\begin{align}
h_{f,l+P,m+P}^{(t)(\nu+1)}&=y_{f,l,m}^{(t)(\nu)}=
h^{(t)(\nu)}_{f,S_P l+j^{(t)(p)}_{_{f,l,m}}+P,S_Pm+k^{(t)(p)}_{_{f,l,m}}+P}.
\end{align}

\begin{figure}[hbt]
	\centering
	\includegraphics[width=13.5cm]{figures/networks/pooling}
	\caption{池化示意图}
	\label{fig:pool}
\end{figure}

深度卷积神经结构具有过拟合的自然趋势，虽然可以通过权重共享来减少参数的数量。但是由于大多数情况下，估计集的数量比训练集大一个数量级，使得神经网络模型的泛化能力不足。在每个训练迭代中，每个隐层单元以预定概率被随机删除，删除后学习过程继续。这些被称作dropout的随机扰动有效地防止了神经网络学习过程的依赖关系，并在隐层单元之间创建了复杂的关系。这样增加了网络模型的复杂度，从而提高深度神经网络模型的泛化能力。


\subsection{激活函数}

在\equref{equ:fp2}中，$f(x)$被用于每一个神经元节点，因此激活函数的选择是构建卷积神经网络模型的一个非常重要的方面，在本节介绍几种常用的激活函数。

\subsubsection{sigmoid 激活函数}

在传统人工神经网络中常用的激活函数为sigmoid激活函数（S型激活函数），其取值范围为$(0,1)$，其定义为：

\begin{align}
f(x)&=\sigma(x)=\frac{1}{1+e^{-x}}.
\end{align}
对其求导可以得到：
\begin{align}
\sigma'(x)&=\sigma(x)\left(1-\sigma(x)\right).
\end{align}

\begin{figure}[hbt]
	\centering
	\includegraphics[width=6.67cm]{figures/networks/sigmoid}
	\caption{sigmoid 激活函数及其导数}
	\label{fig:sigmoid}
\end{figure}

\subsubsection{ tanh 激活函数}

tanh 激活函数的取值范围为 $(-1,1)$，其定义如下：
\begin{align}
f(x)&=\tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}.
\end{align}
对其求导可以得到：
\begin{align}
\tanh'(x)&=1-\tanh^2(x).
\end{align}


\subsubsection{ReLU 激活函数}

ReLU(Rectified Linear Unit)的取值范围为$[0,+\infty)$，其定义为：
\begin{align}
f(x)&={\rm ReLU}(x)=\begin{cases}
      x & x\geq 0 \\
      0& x<0
   \end{cases}.
\end{align}
对其求导可以得到：
\begin{align}
{\rm ReLU}'(x)&=\begin{cases}
      1 & x\geq 0 \\
      0 & x<0
   \end{cases}.
\end{align}


\begin{figure}[hbt]
	\centering
	\begin{minipage}{7cm}
		\includegraphics[width=6.67cm]{figures/networks/tanh2}
    \caption{tanh2 激活函数及其导数}
    \label{fig:tanh2}

	\end{minipage}
	\hspace{10pt}
	\begin{minipage}{7cm}
		\includegraphics[width=6.67cm]{figures/networks/ReLU}
    \caption{ReLU 激活函数及其导数}
    \label{fig:ReLU}

	\end{minipage}

\end{figure}

ReLU具有优于传统激活函数的几个优点：更快的计算速度和更有效的梯度传播（它们不像S形单元那样饱和），生物学可能性和稀疏激活结构。尽管它结构简单，但仍然保持足够的辨别性质。其缺点之一是随机权重的初始状态，多个单元可能过早地落入死区（零输出的恒定梯度）。因此，当与整个连接层进行全连接时，Sigmod激活函数的效果更好。

\subsection{神经网络的学习算法}
参照上文的符号定义，用符号$x $表示训练输入的向量集，$w$表示所有的网络中权重的集合，$b$ 是所有的偏置，用$y=f(w\cdot x + b)$ 表示对应的期望输出。学习算法的主要目的是找到一个权重$w$和偏置$b$使得网络的输出$y$ 可以拟合所有的训练输入$x$。为此可以定义一个损失函数或代价函数(loss function)$C(w,b)$，不同问题可以选取不同损失函数。

从定义可以看出，$C(w,b) $越小说明分类越准确，那么训练神经网络的目的就是找到使得损失函数 $C(w,b)$最小的权重和偏置。
为了求解该问题，将上述问题一般化，原问题变为最小化任意的具有 $m $个变量的多元实值函数 $C(v) $， $v=v_1,v_2,\dots,v_m $。这种具有大量变量的函数的解析解的求解是极其复杂的，一个比较合理的思路为利用数值计算的方法求取其极值点。每次给$C $中的自变量$v$添加一个微小的变化$\Delta v $，根据此变化反映出来的$C $的变化$\Delta C $更新下次的微小变化，从而使得$C $ 可以持续减小。对$C $中自变量的变化$\Delta v=(\Delta v_1,\dots,\Delta v_m)^T $ ， $\Delta C $将会变为
\begin{equation}
\Delta C \approx \nabla C \cdot \Delta v,
    \label{equ:gradient1}
\end{equation}
此处梯度$\nabla C $ 的定义如下：
\begin{equation}
\nabla C \equiv (\frac{\partial C}{\partial v_1},\dots,\frac{\partial C}{\partial v_m})^T,
\end{equation}
其把$v$的变化关联为$C$的变化，假设选取$\Delta v=-\eta \nabla C $，其中的$\eta $是学习率，一般取一个很小的正数，则\equref{equ:gradient1}变为：
\begin{equation}
\Delta C \approx -\eta\nabla C\cdot\nabla C=-\eta||\nabla C||^2 \leq 0.
\end{equation}
也即如利用更新规则$v \rightarrow v'=v-\eta \nabla C$，$C$会持续减小，此更新规则即为最基础的梯度下降算法。
图\ref{fig:gradient-descent}为梯度下降示意图，其跟随当前位置最陡的下降方向也即负梯度，其中学习率$\eta $描述在每个迭代步骤中下降的步长。
可根据选择不同损失函数$C$或通过计算来完成学习率选择等各种技术对学习算法进行优化。
根据梯度下降思想，结合卷积神经网络中的反向传播，获得神经卷积神经网路的训练流程图，如图\ref{fig:cnn_train}所示。


\begin{figure}[hbt]
	\centering
	\begin{minipage}[b][][b]{7cm}
		\includegraphics[width=6.67cm]{figures/networks/gradient-descent}
    \caption{梯度下降示意图}
    \label{fig:gradient-descent}
	\end{minipage}
	\hspace{10pt}
	\begin{minipage}[b][][b]{7cm}
		\includegraphics[width=6.67cm]{figures/networks/cnn_train}
    \caption{神经网络学习算法流程图}
    \label{fig:cnn_train}

	\end{minipage}

\end{figure}

\section{小结}
\label{sec:network_summary}
本章介绍了深度学习的相关基础。首先是其基本分类，然后介绍了传统的神经元的结构和卷积神经网络基于此的改进，最后讨论了深度卷积神经网络的学习算法。
