\chapter{深度卷积神经网络}

深度学习是指具有超过一个隐藏层的神经网络结构，其起始于在2006年发表在顶尖学术刊物《Science》上的一篇文章，可以算作机器学习的一个分支，其于传统的机器学习方法的主要区别为，其通过隐藏层的人工神经网络结构学习得到数据更本质的特征以及通过逐层初始化来克服传统算法在训练上的难度。自从被提出来至今，其在工业界以及学术界掀起了巨大的浪潮，被应用于语音识别、图像识别、自然语音处理和推荐系统等各种方面。

\section{基本分类}
传统上可以把深度学习分为卷积神经网络（CNN）、递归神经网络（RNN）、长短时记忆网络（Long short-term memory, LSTM）,深度信念网络（DBN）等，其中卷积神经网络是最流行的一种深度学习模型。本课题主要利用的是卷积神经网络，通过使用卷积层极大地减少了中间层的参数数目，使学习效率更高并较少过拟合，同时卷积操作独有的局部感受野（local receptive fields）、共享权重（shared weights）和池化（pooling）三种特性也是处理序列元素分类识别的很重要的一点，权重共享策略减少了需要训练的参数，相同的权重可以让滤波器不受信号位置的影响来检测信号的特性，使得训练出来的模型泛化能力更强；池化运算可以降低网络的空间分辨率，从而消除信号的微小偏移和扭曲。

递归神经网络是一种包含循环的，允许信息持久化的神经网络模型。传统的前馈神经网络中，单独的输入完全确定了余下层的神经元的激活值。而对于RNN，隐藏层和输出层的神经元的激活值不仅由当前的网络输入决定，而且包含了前面的输入的影响。
长短时记忆网络是一种RNN特殊的类型，主要用于解决RNN的前期模型难以训练的问题。其通过刻意设计的单元结构，在RNN的基础上添加了元胞状态（cell state）用来保存长期的状态，然后通过门函数来控制此长期状态。

深度信念网络是一个概率生成模型，是由多个限制玻尔兹曼机（Restricted Boltzmann Machines）组成，这些网络被“限制”为一个可见层和一个隐藏层，层间存在连接，但是层内的单元间不存在连接。隐藏单元被训练来捕捉在可见层表现出来的高阶数据的相关性。

从上述的介绍可以看出，CNN是最适合处理本课题这种静态类型的数据，循环或者说是不同时刻的输入对于地海杂波类型的识别并没有提高，故RNN和LSTM显然不适合本问题。另一方面，DBN的生成模型并不关心不同类别之间的最优分类面的位置，故其用于分类问题时，分类精度没有判别模型高。且其学习的是数据的联合分布，相比其他算法具有更高的复杂性。

\section{传统神经网络}

%\begin{table}
%  \caption{测试表格}
%  \centering
%  \begin{tabular}{cccccc}
%    \toprule
%    & $h$ & $L^2$ error & Order & $L^{(\alpha,\beta)}$ error & Order \\
%    \midrule
%    \multirow{5}{4em}{$\alpha=0.85$\\ $\beta = 0.85$}
%    & 1/4   & 3.8571e-04 &      - & 3.0781e-03 &      -\\
%    & 1/8   & 1.3035e-04 & 1.5651 & 1.2640e-03 & 1.2840\\
%    & 1/16  & 3.8665e-05 & 1.7533 & 5.2782e-04 & 1.2599\\
%    & 1/48  & 4.9386e-06 & 1.8731 & 1.5519e-04 & 1.1142\\
%    \bottomrule
%  \end{tabular}
%\end{table}
\section{深度卷积神经网络}
神经网络是有多个图 23中的人工神经元组成，神经元具有多个输入$x_1,x_2,\dots $ ，这些输入可以取0和1中的任意值。神经元对于每一个输入有权重$w_1,w_2,\dots $ 和一个总的偏置$b_0$ ，其输出为一个$\sigma(w\cdot x + b)$ ，这里的 $\sigma$为该神经元的激活函数，定义为：
$\sigma(z)\equiv\frac{1}{1+e^{-z}}$
，此处的激活函数称作S型激活函数，常使用的还有$ReLU $ 等激活函数。

图 27神经元
其基本架构如图 24所示，最左边的为输入层，其中的神经元为输入神经元。最右边的输出层包含有输出神经元，可以有一个也可以由多个。中间层为隐藏层，这部分为需要进行主要设计，也是各种神经网络模型的主要区别之处。为了增强其泛化能力，一般情况下有扩增路径（将多个分支包含在架构中）、金字塔形状（在整个架构中应该有一次整体的平滑的下采样，而且该下采样应该与信道数量的增长结合起来）、规范层输入（使层输入标准化，使所有输入样本更加平等）等各种设计法则，此部分需要根据实际问题以及测试结果不断调整。

\subsection{卷积}
深度卷积神经网络在特征提取过程中一个主要操作为卷积，在前向计算过程中，对于输入的一定区域的数据 和滤波器（或者说权重） 点乘后得到新的特征向量，然后滑过一个个滤波器，组成新的输出数据 。每个滤波器只关心数据的部分特征，当出现它学习到的特征的时候，就会呈现激活态。
$s(t)=\sum_{a=-\infty}^{\infty}x(a)w(t-a) $
\subsection{局部感受野}
：对于传统的神经网络，每个输入元素会连接到每个隐藏神经元。相反，我们只是把输入的频谱数据进行小的、局部区域的连接，也即第一个隐藏层中的每个神经元会连接到一个输入神经元的一个区域。这个输入向量的区域被称为隐藏神经元的局部感受野。它是输入向量上的一个小窗口，对于每个连接学习一个权重而隐藏神经元同时也学习一个总的偏置。通过在整个输入频谱数据上交叉移动局部感受野，可以构建起第一个隐藏层。
\subsection{共享权重和偏置}
上面已经说过对于每个隐藏神经元具有一个偏置和连接到它的局部感受野的权重，同时对于该层的所有的隐藏神经元中每一个使用相同的权重和偏置。也即，对第 个隐藏神经元，输出为：
$\sigma(b+\sum_{m=1}^M w_m a_{j+m}) $
这里 是神经元的激活函数， 是偏置的共享值， 是一个共享权重的 向量， 表示位置 的输入激活值。这意味着第一个隐藏层的所有神经元检测完全相同的特征，只是在输入频谱数据的不同位置，因此卷积网络可以很好地适应频谱数据的布拉格峰偏移情况。

\subsection{池化}
我们在通过卷积获得了特征之后，下一步我们希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如 分类器（多分类的逻辑回归分类器），但这样做面临着计算量的挑战，除此以外过多的特征向量，也容易导致过拟合。

由于我们的杂波频谱数据具有一种“静态性”属性，在一个数据区域有用的特征极有可能在另一个区域同样适用。因此，为了描述数据量较多的数据，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，可以计算频谱数据上一段频率范围内的某个特征的最大值 (或平均值)。这些经过采样的统计特征相比使用所有提取得到的特征不仅具有低得多的维度，同时还不容易过拟合，在一定程度上会改善结果。这种聚合的操作称为池化，常用的池化方法有平均池化和最大池化。 本课题选择频谱向量中的连续范围作为池化区域（池化长度为2），并且只是池化相同的隐藏单元产生的特征。由于这些池化单元具有平移不变性，所以即使频谱数据的布拉格峰经历了一个小的平移之后，依然会产生相同的池化的特征。

深度卷积神经结构具有过拟合的自然趋势，虽然可以通过权重共享来减少参数的数量。但是由于大多数情况下，估计集的数量比训练集大一个数量级，使得神经网络模型的泛化能力不足。在每个训练迭代中，每个隐藏单元以预定概率（我们将其设为 ）被随机删除，删除后学习过程继续。这些被称作 的随机扰动有效地防止了神经网络学习过程的依赖关系，并在隐藏的单元之间创建了复杂的关系。这样增加了网络模型的复杂度，从而提高深度神经网络模型的泛化能力。
\subsection{网络的训练与学习}
如果用符号$x $表示一个训练输入，用$y=y(x) $ 表示对应的期望输出。学习算法的主要目的是，找到一个权重和偏置，使得网络的输出$y(x) $ 可以拟合所有的训练输入$x$ 。为此可以定义一个损失函数（又称作代价函数）：
$C(w,b)\equiv \frac{1}{2n}\sum_x||y(x)-a||^2 $
这里 $w$表示所有的网络中权重的集合，$b$ 是所有的偏置， $n$是训练输入数据的个数，$a$ 是表示当输入为$x$ 时输出的向量，求和总是在总的训练输入$x$ 上进行的。上述损失函数为均方误差，其是可以根据不同的问题进行不同的设置的。从定义可以看出， $C(w,b) $越小说明分类越准确，那么训练神经网络的目的就是找到最小化二次代价函数 $C(w,b) $的权重和偏置。
将上述问题一般化也就是，最小化任意的具有 $m $个变量的多元实值函数 $C(v) $， $v=v_1,v_2,\dots,v_m $。对于这种具有大量变量的函数的解析解是极其复杂的，其比较合理的思路为利用数值计算的方法求取其极值点。每次对于$C $ 中的自变量 添加一个微小的变化$\Delta v $ ，根据此变化反映出来的$C $ 的变换 $\Delta C $更新下次的微小变化，从而使得$C $ 可以持续减小。对$C $ 中自变量的变化$\Delta v=(\Delta v_1,\dots,\Delta v_m)^T $ ， $\Delta C $将会变为$\Delta C \approx \bigtriangledown C \cdot \Delta v $ ，这里的梯度$\bigtriangledown C $ 定义如下：

$\bigtriangledown C \equiv (\frac{\partial C}{\partial v_1},\dots,\frac{\partial C}{\partial v_m})^T $
其把$v $ 的变化关联为$C$ 的变化，假设我们选取
$\Delta v=-\eta \bigtriangledown C $
这里的 $\eta $是一个很小的正数（称为学习速率），这时候有
$\Delta C \approx -\eta\bigtriangledown C\cdot\bigtriangledown C=-\eta||\bigtriangledown C||^2 \leq 0$
也即如果利用更新规则
$v \rightarrow v'=v-\eta \bigtriangledown C$,
$C $会持续减小，此更新规则即为梯度下降算法，这就是最基本的学习算法。可以根据选择不同的代价函数 $C $或者通过计算来完成学习速率的选择等各种技术对学习算法进行优化。

\section{深度卷积神经网络在雷达信号处理中的应用}
